{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c114b7-6751-4192-9939-86d40967caba",
   "metadata": {},
   "source": [
    "# Relationship between income and private health insurance\n",
    "\n",
    "The goal is to investigate the quantitative effect of income on the procurement of private health insurance using US census data. The target of inference is the logistic regression coefficient when regressing the binary indicator of health insurance on income. The data from California in the year 2019 is downloaded through the Folktables interface (1). Predictions of health insurance are made by training a gradient boosting tree via XGBoost (2) on the previous year’s data.\n",
    "\n",
    "1. F. Ding, M. Hardt, J. Miller, L. Schmidt, “Retiring adult: New datasets for fair machine learning” in Advances in Neural Information Processing Systems 34 (2021), pp. 6478–6490.\n",
    "2. T. Chen, C. Guestrin, “XGBoost: A scalable tree boosting system” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016), pp. 785–794."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b89de-40f4-4225-ba6f-f35428d8f648",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec4524b-d6bd-4d3c-ac59-2d6b77ac8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ppi_py.datasets import load_dataset\n",
    "from ppi_py import eff_ppi_logistic_ci_tuned\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import brentq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf90ae6",
   "metadata": {},
   "source": [
    "### Import the census healthcare data set\n",
    "\n",
    "Load the data. The data set contains reported indicators of health insurance (```Y```), predicted indicators of health insurance (```Yhat```), and reported income (```X```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6da3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"./data/\"\n",
    "data = load_dataset(dataset_folder, \"census_healthcare\")\n",
    "Y_total = data[\"Y\"]\n",
    "Yhat_total = data[\"Yhat\"]\n",
    "X_total = data[\"X\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969f9db",
   "metadata": {},
   "source": [
    "### Problem setup\n",
    "\n",
    "Specify the error level (```alpha```), range of values for the labeled data set size (```ns```), and number of trials (```num_trials```).\n",
    "\n",
    "Compute the ground-truth value of the estimand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3c8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "coord = 0  # Choose between 0, 1\n",
    "n_total = Y_total.shape[0]  # Total number of labeled examples\n",
    "ns = np.array([500, 1000, 1500, 2000, 2500]).astype(\n",
    "    int\n",
    ")  # Test for different numbers of labeled ballots\n",
    "num_trials = 100\n",
    "optimizer_options = {\n",
    "    'ftol': 1e-5,\n",
    "    'gtol': 1e-5,\n",
    "    'maxls': 10000,\n",
    "    'maxiter': 10000\n",
    "}\n",
    "# Compute ground truth\n",
    "theta_star = (\n",
    "    LogisticRegression(\n",
    "        penalty=None,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=10000,\n",
    "        tol=1e-15,\n",
    "        fit_intercept=False,\n",
    "        )\n",
    "        .fit(X_total, Y_total)\n",
    "        .coef_.squeeze()\n",
    ")\n",
    "savename = 'census_healthcare_tuned'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce18be",
   "metadata": {},
   "source": [
    "### Construct intervals\n",
    "\n",
    "Form confidence intervals for all methods and problem parameters. A dataframe with the following columns is formed:\n",
    "1. ```method``` (one of ```PPI```, ```Classical```, and ```Imputation```)\n",
    "2. ```n``` (labeled data set size, takes values in ```ns```)\n",
    "3. ```lower``` (lower endpoint of the confidence interval)\n",
    "4. ```upper``` (upper endpoint of the confidence interval)\n",
    "5. ```trial``` (index of trial, goes from ```0``` to ```num_trials-1```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▌                                                                                       | 6/100 [08:20<2:10:26, 83.26s/it]"
     ]
    }
   ],
   "source": [
    "# Run prediction-powered inference and classical inference for many values of n\n",
    "results = []\n",
    "for j in tqdm(range(num_trials)):\n",
    "    for i in range(ns.shape[0]):\n",
    "        # Prediction-Powered Inference\n",
    "        n = ns[i]\n",
    "        rand_idx = np.random.permutation(n_total)\n",
    "        _X, _X_unlabeled = X_total[rand_idx[:n]], X_total[rand_idx[n:]]\n",
    "        _Y, _Y_unlabeled = Y_total[rand_idx[:n]], Y_total[rand_idx[n:]]\n",
    "        _Yhat, _Yhat_unlabeled = (\n",
    "            Yhat_total[rand_idx[:n]],\n",
    "            Yhat_total[rand_idx[n:]],\n",
    "        )\n",
    "        ppi_ci_tuned = eff_ppi_logistic_ci_tuned(_X, _Y, _Yhat, _X_unlabeled, _Yhat_unlabeled, alpha=alpha, coord=coord, optimizer_options=optimizer_options)\n",
    "        ppi_ci = eff_ppi_logistic_ci_tuned(_X, _Y, _Yhat, _X_unlabeled, _Yhat_unlabeled, alpha=alpha, lhat=1, optimizer_options=optimizer_options)\n",
    "        classical_ci = eff_ppi_logistic_ci_tuned(_X, _Y, _Yhat, _X_unlabeled, _Yhat_unlabeled, alpha=alpha, lhat=0, optimizer_options=optimizer_options)\n",
    "\n",
    "        # Append results\n",
    "        results += [\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"method\": \"PPI\",\n",
    "                        \"n\": n,\n",
    "                        \"lower\": ppi_ci[0][coord],\n",
    "                        \"upper\": ppi_ci[1][coord],\n",
    "                        \"included\": (ppi_ci_tuned[0][coord] <= theta_star[coord]) & (ppi_ci_tuned[1][coord] >= theta_star[coord]),\n",
    "                        \"trial\": j,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        results += [\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"method\": \"classical\",\n",
    "                        \"n\": n,\n",
    "                        \"lower\": classical_ci[0][coord],\n",
    "                        \"upper\": classical_ci[1][coord],\n",
    "                        \"included\": (classical_ci[0][coord] <= theta_star[coord]) & (classical_ci[1][coord] >= theta_star[coord]),\n",
    "                        \"trial\": j,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        results += [\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"method\": \"tuned PPI\",\n",
    "                        \"n\": n,\n",
    "                        \"lower\": ppi_ci_tuned[0][coord],\n",
    "                        \"upper\": ppi_ci_tuned[1][coord],\n",
    "                        \"included\": (ppi_ci_tuned[0][coord] <= theta_star[coord]) & (ppi_ci_tuned[1][coord] >= theta_star[coord]),\n",
    "                        \"trial\": j,\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "df = pd.concat(results, axis=0, ignore_index=True)\n",
    "df[\"width\"] = df[\"upper\"] - df[\"lower\"]\n",
    "os.makedirs('.cache/', exist_ok=True)\n",
    "df.to_csv('.cache/' + savename + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {'tuned PPI' : 'darkorange',\n",
    "           'PPI' : '#83C980',\n",
    "           'classical' : 'cornflowerblue'}\n",
    "linewidth=2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,2), sharex=True, sharey=False)\n",
    "cvg_ax = axs[0]\n",
    "sz_ax = axs[1]\n",
    "sns.lineplot(ax=cvg_ax, data=df, x=\"n\", y=\"included\", linewidth=linewidth, errorbar=None, hue=\"method\", legend=False, palette=palette, alpha=0.8)\n",
    "sns.lineplot(ax=sz_ax, data=df, x=\"n\", y=\"width\", linewidth=linewidth, errorbar=None, hue=\"method\", legend=True, palette=palette, alpha=0.8)\n",
    "cvg_ax.set_ylabel(\"coverage\")\n",
    "cvg_ax.set_ylim([0.5,1.03])\n",
    "cvg_ax.axhline(y=1-alpha, color=\"#888888\", linestyle='dotted')\n",
    "cvg_ax.set_xlabel(\"n\")\n",
    "sz_ax.set_ylabel(\"size\")\n",
    "sz_ax.set_xlabel(\"n\")\n",
    "sz_ax.legend_.set_title(None)\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "os.makedirs('./plots',exist_ok=True)\n",
    "plt.savefig('./plots/' + savename + '.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
