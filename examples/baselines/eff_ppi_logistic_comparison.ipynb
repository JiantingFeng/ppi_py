{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6be9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ppi_py.datasets import load_dataset\n",
    "from ppi_py import ppi_logistic_ci, eff_ppi_logistic_ci\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import brentq\n",
    "from scipy.special import expit\n",
    "from baseline_utils import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ae754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [02:35<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.array([0.2, 0.1, 0.05])\n",
    "n = 1000\n",
    "N = 10000\n",
    "d = 2\n",
    "num_trials = 100\n",
    "eff_includeds = np.zeros((num_trials, len(alphas)))\n",
    "eff_sizes = np.zeros((num_trials, len(alphas), d))\n",
    "includeds = np.zeros((num_trials, len(alphas)))\n",
    "sizes = np.zeros((num_trials, len(alphas), d))\n",
    "for i in tqdm(range(num_trials)):\n",
    "    # Make a synthetic regression problem\n",
    "    X = np.random.randn(n, d)\n",
    "    beta = 5*np.random.randn(d)\n",
    "    beta_prediction = beta + np.random.randn(d) + 2#+ np.random.randn(d) + 2\n",
    "    Y = np.random.binomial(1, expit(X.dot(beta)))\n",
    "    Yhat = expit(X.dot(beta_prediction))\n",
    "    # Make a synthetic unlabeled data set with predictions Yhat\n",
    "    X_unlabeled = np.random.randn(N, d)\n",
    "    Yhat_unlabeled = expit(X_unlabeled.dot(beta_prediction))\n",
    "    # Compute the confidence interval\n",
    "    for j in range(len(alphas)):\n",
    "        eff_ppi_ci = eff_ppi_logistic_ci(\n",
    "            X, Y, Yhat, X_unlabeled, Yhat_unlabeled, alpha=alphas[j], grad_tol=1e-1\n",
    "        )\n",
    "        ppi_ci = ppi_logistic_ci(\n",
    "            X, Y, Yhat, X_unlabeled, Yhat_unlabeled, alpha=alphas[j], grad_tol=1e-1, grid_size=2000,\n",
    "        )\n",
    "        eff_sizes[i,j,:] = np.array([ eff_ppi_ci[1][_d] - eff_ppi_ci[0][_d] for _d in range(d) ])\n",
    "        sizes[i,j,:] = np.array([ ppi_ci[1][_d] - ppi_ci[0][_d] for _d in range(d) ])\n",
    "        # Check that the confidence interval contains the true beta\n",
    "        eff_includeds[i,j] = int(\n",
    "            (eff_ppi_ci[0][0] <= beta[0]) & (beta[0] <= eff_ppi_ci[1][0])\n",
    "        )\n",
    "        includeds[i,j] = int(\n",
    "            (ppi_ci[0][0] <= beta[0]) & (beta[0] <= ppi_ci[1][0])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c49f0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.05925926 2.24      ]\n",
      " [2.30666667 2.48      ]\n",
      " [2.44       2.62074074]]\n",
      "\n",
      "[[1.25418676 1.43103425]\n",
      " [1.60973128 1.83671258]\n",
      " [1.91811312 2.18857802]]\n",
      "\n",
      "[0.96 0.98 0.98]\n",
      "\n",
      "[0.92 0.94 0.97]\n"
     ]
    }
   ],
   "source": [
    "print(sizes.mean(axis=0))\n",
    "print()\n",
    "print(eff_sizes.mean(axis=0))\n",
    "print()\n",
    "print(includeds.mean(axis=0))\n",
    "print()\n",
    "print(eff_includeds.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d7b7b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(includeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
